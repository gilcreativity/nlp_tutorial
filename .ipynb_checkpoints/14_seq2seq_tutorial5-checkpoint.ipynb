{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = []\n",
    "korean = []\n",
    "count = 50\n",
    "with open('korean-english-park.train.en', 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        english.append(line)\n",
    "        if i-1 == count:\n",
    "            break\n",
    "\n",
    "with open('korean-english-park.train.ko', 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        korean.append(line)\n",
    "        if i-1 == count:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(english)):\n",
    "    english[i] = re.sub('\\n', '', english[i])\n",
    "for i in range(len(korean)):\n",
    "    korean[i] = re.sub('\\n', '', korean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(english)):\n",
    "    english[i] = english[i].split()\n",
    "for i in range(len(korean)):\n",
    "    korean[i] = korean[i].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data = np.stack((english, korean), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_arr = []\n",
    "for seq in english:\n",
    "    word_arr += seq\n",
    "word_arr += ['<P>']\n",
    "en_word2num = {c:i for i, c in enumerate(set(word_arr))}\n",
    "en_num2word = {i:c for i, c in enumerate(en_word2num.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_arr = []\n",
    "for seq in korean:\n",
    "    word_arr += seq\n",
    "word_arr += ['<S>', '</S>', '<P>']\n",
    "ko_word2num = {c:i for i, c in enumerate(set(word_arr))}\n",
    "ko_num2word = {i:c for i, c in enumerate(ko_word2num.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(seq_data):\n",
    "    max_len = 0\n",
    "    for seq in seq_data:\n",
    "        if max_len < len(seq):\n",
    "            max_len = len(seq)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data, enc_max_len, dec_max_len):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "    for i, seq in enumerate(seq_data):\n",
    "        input = []\n",
    "        output = []\n",
    "        target = []\n",
    "        for token in seq[0]:\n",
    "            input.append(en_word2num[token])\n",
    "        for _ in range(len(seq[0]), enc_max_len):\n",
    "            input.append(en_word2num['<P>'])\n",
    "        input_batch.append(input)\n",
    "        output.append(ko_word2num['<S>'])\n",
    "        for token in seq[1]:\n",
    "            output.append(ko_word2num[token])\n",
    "            target.append(ko_word2num[token])\n",
    "        target.append(ko_word2num['</S>'])\n",
    "        for _ in range(len(seq[1]), dec_max_len):\n",
    "            output.append(ko_word2num['</S>'])\n",
    "            target.append(ko_word2num['</S>'])\n",
    "                \n",
    "        output_batch.append(output)\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_length(seq_data):\n",
    "    seq_len = []\n",
    "    for i, seq in enumerate(seq_data):\n",
    "        seq_len.append(len(seq))\n",
    "    return seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "n_hidden = 128\n",
    "max_enc_step = get_max_length(english)\n",
    "max_dec_step = get_max_length(korean)\n",
    "n_embedding = 300\n",
    "total_epoch = 500\n",
    "batch_size = count\n",
    "en_dic_len = len(en_word2num)\n",
    "ko_dic_len = len(ko_word2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "enc_input = tf.placeholder(tf.int32, [None, max_enc_step])\n",
    "dec_input = tf.placeholder(tf.int32, [None, max_dec_step+1])\n",
    "targets = tf.placeholder(tf.int64, [None, max_dec_step+1])\n",
    "W = tf.get_variable(name='encode_embedding', shape=[en_dic_len, n_embedding], trainable=True)\n",
    "W_ = tf.get_variable(name='decode_embedding', shape=[ko_dic_len, n_embedding], trainable=True)\n",
    "enc_seq_len = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "dec_seq_len = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "enc_inputs = tf.nn.embedding_lookup(W, enc_input)\n",
    "dec_inputs = tf.nn.embedding_lookup(W_, dec_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    #enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_inputs, sequence_length=enc_seq_len, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    #dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_inputs, initial_state = enc_states, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(outputs, ko_dic_len, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(44), Dimension(128)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(logits=logits, targets=targets, weights=tf.sequence_mask(dec_seq_len+1, max_dec_step+1, dtype=tf.float32)))\n",
    "predict = tf.argmax(logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 050 cost = 0.091423\n",
      "epoch: 100 cost = 0.036631\n",
      "epoch: 150 cost = 0.020256\n",
      "epoch: 200 cost = 0.012518\n",
      "epoch: 250 cost = 0.006923\n",
      "epoch: 300 cost = 0.003601\n",
      "epoch: 350 cost = 0.001977\n",
      "epoch: 400 cost = 0.001213\n",
      "epoch: 450 cost = 0.000822\n",
      "epoch: 500 cost = 0.000598\n",
      "optimization finished!\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "total_batch = int(len(seq_data)/batch_size)\n",
    "for epoch in range(total_epoch):\n",
    "    loss_sum = 0\n",
    "    #print('< epoch:', epoch+1, '>')\n",
    "    for i in range(total_batch):\n",
    "        if i == (total_batch-1):\n",
    "            input_batch, output_batch, target_batch = make_batch(seq_data[i*batch_size:len(seq_data)], max_enc_step, max_dec_step)\n",
    "            enc_seq_data = get_seq_length(english[i*batch_size:len(seq_data)])\n",
    "            dec_seq_data = get_seq_length(korean[i*batch_size:len(seq_data)])\n",
    "        else:\n",
    "            input_batch, output_batch, target_batch = make_batch(seq_data[i*batch_size:(i+1)*batch_size], max_enc_step, max_dec_step)\n",
    "            enc_seq_data = get_seq_length(english[i*batch_size:(i+1)*batch_size])\n",
    "            dec_seq_data = get_seq_length(korean[i*batch_size:(i+1)*batch_size])\n",
    "        \n",
    "        _, loss = sess.run([optimizer, cost], feed_dict={enc_input: input_batch, dec_input: output_batch, targets: target_batch, enc_seq_len: enc_seq_data, dec_seq_len: dec_seq_data})\n",
    "        loss_sum += loss\n",
    "        #if i % 30 == 29:\n",
    "        #    print('batch:', '%03d' % (i+1), 'cost =', '{:.6f}'.format(loss_sum/30))\n",
    "        #    loss_sum = 0\n",
    "    if epoch % 50 == 49:\n",
    "        print('epoch:', '%03d' % (epoch+1), 'cost =', '{:.6f}'.format(loss_sum/50))\n",
    "        \n",
    "print('optimization finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(english):\n",
    "    english = [english]\n",
    "    korean = [['<P>']*max_dec_step]\n",
    "    seq_data = [english + korean]\n",
    "    input_batch, output_batch, target_batch = make_batch(seq_data, max_enc_step, max_dec_step)\n",
    "    enc_seq_data = get_seq_length(english)\n",
    "    dec_seq_data = get_seq_length(korean)\n",
    "    result = sess.run(predict, feed_dict={enc_input: input_batch, dec_input: output_batch, targets: target_batch, enc_seq_len: enc_seq_data, dec_seq_len: dec_seq_data})\n",
    "    decoded = [ko_num2word[j] for j in np.squeeze(result)]\n",
    "    end = len(decoded)-1\n",
    "    if '</S>' in decoded:\n",
    "        end = decoded.index('</S>')\n",
    "    translated = ' '.join(decoded[:end])\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Much of personal computing is about \"can you top this?\"  \n",
      "-> 개인용 컴퓨터 컴퓨터 사용의 사용의 사용의 사용의 루이스 껌에 껌에 껌에 가운데, 가운데, 의회가 설립 설립 실바가 실바가 고도가 고도가 고도가 고도가 고도가 고도가 고도가 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 루이스 루이스 실바가 실바가 실바가 실바가 실바가 실바가 실바가 \n",
      "\n",
      "so a mention a few weeks ago about a rechargeable wireless optical mouse brought in another rechargeable, wireless mouse.  \n",
      "-> 모든 광마우스와 마찬가지 마찬가지 로 광마우스도 광마우스도 광마우스도 위에 위에 위에 위에 모여 들어, 실바가 실바가 실바가 실바가 고도가 고도가 고도가 고도가 고도가 고도가 곳에서 곳에서 곳에서 연구에서 연구에서 연구에서 연구에서 연구에서 \n",
      "\n",
      "Like all optical mice, But it also doesn't need a desk.  \n",
      "-> 그러나 이것은 이것은 또한 책상도 책상도 책상도 거세지고 거세지고 가운데, 가운데, 고도가 고도가 고도가 고도가 회담을 회담을 회담을 회담을 동남 동남 수 \n",
      "\n",
      "uses gyroscopic sensors to control the cursor movement as you move your wrist, arm, whatever through the air.  \n",
      "-> 79.95달러하는 79.95달러하는 최첨단 최첨단 광마우스는 광마우스는 광마우스는 광마우스는 광마우스는 실바가 실바가 실바가 실바가 실바가 실바가 실바가 실바가 투표에서 투표에서 곳에서 곳에서 곳에서 곳에서 약해진 약해진 약해진 약해진 약해진 약해진 약해진 약해진 약해진 실바가 실바가 실바가 실바가 실바가 실바가 실바가 실바가 실바가 실바가 실바가 \n",
      "\n",
      "Intelligence officials have revealed a spate of foiled plots on ships in Southeast Asia and are warning that a narrow stretch of water carrying almost one third of the world's maritime trade is vulnerable to a terror attack.  \n",
      "-> 정보 관리들은 관리들은 아시아에서의 아시아에서의 선박들에 선박들에 선박들에 (테러) (테러) (테러) 돌아갔음을 돌아갔음을 돌아갔음을 건이 실패했다는 실패했다는 실패했다는 실패했다는 위해 위해 위해 250억달러에 250억달러에 250억달러에 250억달러에 250억달러에 250억달러에 250억달러에 경고하고 경고하고 경고하고 경고하고 향상시키는데 향상시키는데 향상시키는데 향상시키는데 있습니다.\" 있습니다.\" 있습니다.\" 있습니다.\" 있다. 있다. \n",
      "\n",
      "After learning of several foiled al Qaeda attempts on U.S. and commercial ships in the area, experts are warning that the terror network still wants to cripple the global economy, the world's economic jugular vein in Southeast Asia is at risk.  \n",
      "-> 이 지역에 있는 있는 선박과 선박과 상업용 선박들에 알카에다의 알카에다의 알카에다의 시도 중 중 중 쟁점들을 검토하기 검토하기 실패했다는 보다 보다 보다 후에, 후에, 후에, 후에, 후에, 후에, 후에, 후에, 후에, 후에, 후에, 여전히 경고하고 경고하고 경고하고 이번 이번 이번 이번 이번 이번 \n",
      "\n",
      "Caffeine can help increase reaction time and improve performance for military servicemen who must perform complex tasks or who need help staying alert for longer periods of time, according to a new report by the National Academy of Sciences.  \n",
      "-> 국립 과학 학회가 발표한 발표한 발표한 복잡한 복잡한 수행해야 수행해야 수행해야 수행해야 수행해야 수행해야 보다 보다 보다 보다 위해 위해 위해 운송하는 운송하는 반응 말라카 말라카 도움이 도움이 도움이 증가시키고 수행 수행 향상시키는데 향상시키는데 향상시키는데 향상시키는데 향상시키는데 향상시키는데 이번 이번 이번 이번 \n",
      "\n",
      "\"Specifically, it can be used in maintaining speed of reactions and visual and auditory vigilance, which in military operations could be a life or death situation,\" according to the report.  \n",
      "-> 이 보고서에따르면, 보고서에따르면, 군사 작전에서 작전에서 걸린 걸린 걸린 수행해야 수행해야 5명이 5명이 부상당하고, 부상당하고, 부상당하고, 실패했다는 위해 위해 위해 위해 위해 위해 위해 여전히 여전히 여전히 여전히 경고하고 경고하고 경고하고 경고하고 이번 이번 이번 이번 \n",
      "\n",
      "\"Something that will boost their capabilities at crucial moments is very important.\"  \n",
      "-> \"결정적인 순간에 순간에 그들의 그들의 증가시켜 줄 줄 그외에 들어, 실바가 붉은 붉은 붉은 붉은 민간 곳에서 곳에서 곳에서 흔들며 구출되었다. 구출되었다. 있다. 있다. 있다. 있다. 있다. 있다. 있다. \n",
      "\n",
      "Researchers are already exploring ways to put caffeine in nutrition bars or chewing gum as alternatives to coffee, Archibald said.  \n",
      "-> 연구가들이 이미 커피 커피 커피 대체품으로서 과자나 과자나 껌에 껌에 껌에 껌에 껌에 껌에 껌에 결선 결선 결선 결선 결선 결선 결선 고도가 고도가 고도가 고도가 고도가 곳에서 곳에서 곳에서 곳에서 곳에서 실바가 실바가 실바가 실바가 실바가 실바가 실바가 실바가 실바가 실바가 실바가 \n",
      "\n",
      "A similar dose of caffeine, about 200-600 mg, also appears effective in enhancing physical endurance and may be especially useful in returning some of the physical endurance lost at high altitude, the study found.  \n",
      "-> 약 200600밀리그램의, 비슷한 비슷한 분량의 카페인은 카페인은 카페인은 강화시키는 강화시키는 강화시키는 강화시키는 강화시키는 강화시키는 의회가 의회가 의회가 의회가 고도가 고도가 고도가 고도가 고도가 고도가 고도가 고도가 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 곳에서 실바가 실바가 실바가 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, seq in enumerate(english):\n",
    "    temp = ''\n",
    "    for token in seq:\n",
    "        temp += token + ' '\n",
    "    #print(temp)\n",
    "    print(temp, '\\n->', translate(seq), '\\n')\n",
    "    if i == 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensor",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
