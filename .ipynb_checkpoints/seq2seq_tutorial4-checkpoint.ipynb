{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = []\n",
    "korean = []\n",
    "count = 200\n",
    "with open('korean-english-park.train.en', 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        english.append(line)\n",
    "        if i-1 == count:\n",
    "            break\n",
    "\n",
    "with open('korean-english-park.train.ko', 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        korean.append(line)\n",
    "        if i-1 == count:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(english)):\n",
    "    english[i] = re.sub('\\n', '', english[i])\n",
    "for i in range(len(korean)):\n",
    "    korean[i] = re.sub('\\n', '', korean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(english)):\n",
    "    english[i] = english[i].split()\n",
    "for i in range(len(korean)):\n",
    "    korean[i] = korean[i].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data = np.stack((english, korean), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_arr = []\n",
    "for seq in english:\n",
    "    word_arr += seq\n",
    "word_arr += ['<P>']\n",
    "en_word2num = {c:i for i, c in enumerate(set(word_arr))}\n",
    "en_num2word = {i:c for i, c in enumerate(en_word2num.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_arr = []\n",
    "for seq in korean:\n",
    "    word_arr += seq\n",
    "word_arr += ['<S>', '</S>', '<P>']\n",
    "ko_word2num = {c:i for i, c in enumerate(set(word_arr))}\n",
    "ko_num2word = {i:c for i, c in enumerate(ko_word2num.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(seq_data):\n",
    "    max_len = 0\n",
    "    for seq in seq_data:\n",
    "        if max_len < len(seq):\n",
    "            max_len = len(seq)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data, enc_max_len, dec_max_len):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "    for i, seq in enumerate(seq_data):\n",
    "        input = []\n",
    "        output = []\n",
    "        target = []\n",
    "        for token in seq[0]:\n",
    "            input.append(en_word2num[token])\n",
    "        for _ in range(len(seq[0]), enc_max_len):\n",
    "            input.append(en_word2num['<P>'])\n",
    "        input_batch.append(input)\n",
    "        output.append(ko_word2num['<S>'])\n",
    "        for token in seq[1]:\n",
    "            output.append(ko_word2num[token])\n",
    "            target.append(ko_word2num[token])\n",
    "        target.append(ko_word2num['</S>'])\n",
    "        for _ in range(len(seq[1]), dec_max_len):\n",
    "            output.append(ko_word2num['</S>'])\n",
    "            target.append(ko_word2num['</S>'])\n",
    "                \n",
    "        output_batch.append(output)\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_length(seq_data):\n",
    "    seq_len = []\n",
    "    for i, seq in enumerate(seq_data):\n",
    "        seq_len.append(len(seq))\n",
    "    return seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "n_hidden = 128\n",
    "max_enc_step = get_max_length(english)\n",
    "max_dec_step = get_max_length(korean)\n",
    "n_embedding = 300\n",
    "total_epoch = 500\n",
    "batch_size = count\n",
    "en_dic_len = len(en_word2num)\n",
    "ko_dic_len = len(ko_word2num)\n",
    "n_input = len(en_word2num)\n",
    "n_class = len(ko_word2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "enc_input = tf.placeholder(tf.int32, [None, max_enc_step])\n",
    "dec_input = tf.placeholder(tf.int32, [None, max_dec_step+1])\n",
    "dec_inputs = tf.one_hot(dec_input, ko_dic_len)\n",
    "W = tf.get_variable(name='embedding', shape=[en_dic_len, n_embedding], trainable=True)\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "enc_seq_len = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "dec_seq_len = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "enc_inputs = tf.nn.embedding_lookup(W, enc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    #enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_inputs, sequence_length=enc_seq_len, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    #dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_inputs, initial_state = enc_states, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(outputs, ko_dic_len, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense/BiasAdd:0' shape=(?, 44, 2526) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(logits=logits, targets=targets, weights=tf.sequence_mask(dec_seq_len+1, max_dec_step+1, dtype=tf.float32)))\n",
    "predict = tf.argmax(logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 050 cost = 0.133571\n",
      "epoch: 100 cost = 0.119447\n",
      "epoch: 150 cost = 0.096871\n",
      "epoch: 200 cost = 0.065664\n",
      "epoch: 250 cost = 0.040211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-d2a07d9f2d4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mdec_seq_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_seq_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkorean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0menc_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moutput_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_seq_len\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0menc_seq_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_seq_len\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdec_seq_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mloss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m#if i % 30 == 29:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "total_batch = int(len(seq_data)/batch_size)\n",
    "for epoch in range(total_epoch):\n",
    "    loss_sum = 0\n",
    "    #print('< epoch:', epoch+1, '>')\n",
    "    for i in range(total_batch):\n",
    "        if i == (total_batch-1):\n",
    "            input_batch, output_batch, target_batch = make_batch(seq_data[i*batch_size:len(seq_data)], max_enc_step, max_dec_step)\n",
    "            enc_seq_data = get_seq_length(english[i*batch_size:len(seq_data)])\n",
    "            dec_seq_data = get_seq_length(korean[i*batch_size:len(seq_data)])\n",
    "        else:\n",
    "            input_batch, output_batch, target_batch = make_batch(seq_data[i*batch_size:(i+1)*batch_size], max_enc_step, max_dec_step)\n",
    "            enc_seq_data = get_seq_length(english[i*batch_size:(i+1)*batch_size])\n",
    "            dec_seq_data = get_seq_length(korean[i*batch_size:(i+1)*batch_size])\n",
    "        \n",
    "        _, loss = sess.run([optimizer, cost], feed_dict={enc_input: input_batch, dec_input: output_batch, targets: target_batch, enc_seq_len: enc_seq_data, dec_seq_len: dec_seq_data})\n",
    "        loss_sum += loss\n",
    "        #if i % 30 == 29:\n",
    "        #    print('batch:', '%03d' % (i+1), 'cost =', '{:.6f}'.format(loss_sum/30))\n",
    "        #    loss_sum = 0\n",
    "    if epoch % 50 == 49:\n",
    "        print('epoch:', '%03d' % (epoch+1), 'cost =', '{:.6f}'.format(loss_sum/50))\n",
    "        \n",
    "print('optimization finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(english):\n",
    "    english = [english]\n",
    "    korean = [['<P>']*max_dec_step]\n",
    "    seq_data = [english + korean]\n",
    "    input_batch, output_batch, target_batch = make_batch(seq_data, max_enc_step, max_dec_step)\n",
    "    enc_seq_data = get_seq_length(english)\n",
    "    dec_seq_data = get_seq_length(korean)\n",
    "    result = sess.run(predict, feed_dict={enc_input: input_batch, dec_input: output_batch, targets: target_batch, enc_seq_len: enc_seq_data, dec_seq_len: dec_seq_data})\n",
    "    decoded = [ko_num2word[j] for j in np.squeeze(result)]\n",
    "    end = len(decoded)-1\n",
    "    if '</S>' in decoded:\n",
    "        end = decoded.index('</S>')\n",
    "    translated = ' '.join(decoded[:end])\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Much of personal computing is about \"can you top this?\"  \n",
      "-> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 있느냐?\" \n",
      "\n",
      "so a mention a few weeks ago about a rechargeable wireless optical mouse brought in another rechargeable, wireless mouse.  \n",
      "-> 이 광마우스와 마찬가지 로 광마우스도 광마우스도 책상 위에 놓는 마우스 패드를 하지 \n",
      "\n",
      "Like all optical mice, But it also doesn't need a desk.  \n",
      "-> 그러나 이것은 또한 책상도 필요로 필요로 \n",
      "\n",
      "uses gyroscopic sensors to control the cursor movement as you move your wrist, arm, whatever through the air.  \n",
      "-> 79.95달러하는 최첨단 최첨단 무선 광마우스는 허공에서 팔목, 팔, 그외에 어떤 부분이든 부분이든 움직임에따라 커서의 움직임을 회전 회전 \n",
      "\n",
      "Intelligence officials have revealed a spate of foiled plots on ships in Southeast Asia and are warning that a narrow stretch of water carrying almost one third of the world's maritime trade is vulnerable to a terror attack.  \n",
      "-> 이 관리들은 동남 아시아에서의 선박들에 대한 (테러) (테러) 계획들이 실패로 돌아갔음을 돌아갔음을 해상 해상 교역량의 거의 3분의 1을 1을 운송하는 좁은 말라카 말라카 \n",
      "\n",
      "After learning of several foiled al Qaeda attempts on U.S. and commercial ships in the area, experts are warning that the terror network still wants to cripple the global economy, the world's economic jugular vein in Southeast Asia is at risk.  \n",
      "-> 이 지역에 있는 선박과 선박과 상업용 선박들에 대한 알카에다의 (테러) 시도 중 건이 건이 실패했다는 알게 된 된 된 전문가들은 전문가들은 조직이 조직이 여전히 \n",
      "\n",
      "Caffeine can help increase reaction time and improve performance for military servicemen who must perform complex tasks or who need help staying alert for longer periods of time, according to a new report by the National Academy of Sciences.  \n",
      "-> 국립 과학 학회가 발표한 새 보고서에따르면, 복잡한 임무를 수행해야 군인들이나 군인들이나 군인들이나 오랜 오랜 경계를 늦추지 늦추지 않고 있기 있기 있기 필요한 도움이 도움이 \n",
      "\n",
      "\"Specifically, it can be used in maintaining speed of reactions and visual and auditory vigilance, which in military operations could be a life or death situation,\" according to the report.  \n",
      "-> 이 \"특히, \"특히, 작전에서 작전에서 생사가 걸린 상황이 상황이 있는 있는 속도와 속도와 시각 시각 청각의 경계 상태를 상태를 유지시키기 카페인이 카페인이 \n",
      "\n",
      "\"Something that will boost their capabilities at crucial moments is very important.\"  \n",
      "-> 그러나 순간에 그들의 능력을 증가시켜 줄 무엇이 무엇이 무엇이 중요합니다.\" \n",
      "\n",
      "Researchers are already exploring ways to put caffeine in nutrition bars or chewing gum as alternatives to coffee, Archibald said.  \n",
      "-> 연구가들이 이미 커피 대체품으로서 음식 대용 과자나 껌에 카페인을 첨가하는 방법을 연구하고 Archibald는 Archibald는 \n",
      "\n",
      "A similar dose of caffeine, about 200-600 mg, also appears effective in enhancing physical endurance and may be especially useful in returning some of the physical endurance lost at high altitude, the study found.  \n",
      "-> 이 200600밀리그램의, 비슷한 분량의 카페인은 카페인은 육체적 지구력을 강화시키는 강화시키는 효과적인 효과적인 같으며, 고도가 고도가 고도가 곳에서 약해진 약해진 촉구하는 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, seq in enumerate(english):\n",
    "    temp = ''\n",
    "    for token in seq:\n",
    "        temp += token + ' '\n",
    "    #print(temp)\n",
    "    print(temp, '\\n->', translate(seq), '\\n')\n",
    "    if i == 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensor",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
